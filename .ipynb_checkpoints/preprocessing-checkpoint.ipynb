{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Script\n",
    "\n",
    "This script takes train.csv and test.csv as input and generates the following preprocessed output files:\n",
    "\n",
    "- p_train.csv - training data\n",
    "- p_test.csv - test data\n",
    "- p_train_y_nn.csv - training data labels processed for neural network\n",
    "- p_train_y_label.csv - training data labels processed for tree-based models\n",
    "- p_train_std.csv - standardized training data for neural network\n",
    "- p_test_std.csv - standardized test data for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "\n",
    "import gensim\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', low_memory=False)  # update path as needed\n",
    "test_df = pd.read_csv('test.csv', low_memory=False) # update path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Features from 'Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dates(df):\n",
    "    \n",
    "    df['Dates'] = df['Dates'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "    df['Year'] = df['Dates'].apply(lambda x: x.year)\n",
    "    df['Month'] = df['Dates'].apply(lambda x: x.month)\n",
    "    df['Day'] = df['Dates'].apply(lambda x: x.day)\n",
    "    df['Hour'] = df['Dates'].apply(lambda x: x.hour)\n",
    "    df['Minute'] = df['Dates'].apply(lambda x: x.minute)\n",
    "    df['Special Time'] = df['Minute'].isin([0,30]).astype(int)\n",
    "    df.drop('Dates', axis=1, inplace=True)\n",
    "    \n",
    "    df['Weekend'] = df['DayOfWeek'].apply(lambda x: 1 if x == 'Saturday' or x == 'Sunday' else 0)\n",
    "    df['Night'] = df['Hour'].apply(lambda x: 1 if x > 6 and x < 18 else 0)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicate Training Entries and Unuseful Columns, Manage Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "train_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage outliers\n",
    "train_df.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "test_df.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "\n",
    "for district in train_df['PdDistrict'].unique():\n",
    "    train_df.loc[train_df['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n",
    "        train_df.loc[train_df['PdDistrict'] == district, ['X', 'Y']])\n",
    "    test_df.loc[test_df['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n",
    "        test_df.loc[test_df['PdDistrict'] == district, ['X', 'Y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unuseful columns \n",
    "\n",
    "drop_cols = ['Descript', 'Resolution', 'Id']\n",
    "\n",
    "for col in drop_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df.drop(col, axis=1, inplace=True)\n",
    "    if col in test_df.columns:\n",
    "        test_df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "X = train_df.drop('Category', axis=1)\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cats = train_df['Category']\n",
    "unique_cats = np.sort(y_cats.unique())\n",
    "\n",
    "# neural network\n",
    "y = np.zeros((y_cats.shape[0], 39))\n",
    "for idx, target in enumerate(list(y_cats)):\n",
    "    y[idx, np.where(unique_cats == target)] = 1\n",
    "\n",
    "y_nn = pd.DataFrame(y, columns = unique_cats)\n",
    "\n",
    "\n",
    "# tree-based models\n",
    "y_label = train_df['Category']\n",
    "le = LabelEncoder()\n",
    "y_label = le.fit_transform(y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = X.shape[0]\n",
    "\n",
    "combined = pd.concat([X, X_test], ignore_index=True)\n",
    "combined = process_dates(combined)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Address Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_list = [address.split(' ') for address in combined['Address']]\n",
    "address_model = gensim.models.Word2Vec(address_list, min_count=1)\n",
    "encoded_address = np.zeros((combined.shape[0], 100))\n",
    "for i in range(len(address_list)):\n",
    "    for j in range(len(address_list[i])):\n",
    "        encoded_address[i] += address_model.wv[address_list[i][j]]\n",
    "    encoded_address[i] /= len(address_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Intersection'] = combined['Address'].apply(lambda x: 1 if '/' in x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations & Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xy_scaler = StandardScaler()\n",
    "# xy_scaler.fit(combined[['X', 'Y']])\n",
    "# combined[['X', 'Y']] = xy_scaler.transform(combined[['X', 'Y']])\n",
    "\n",
    "X_median = combined[\"X\"].median()\n",
    "Y_median = combined[\"Y\"].median()\n",
    "\n",
    "combined[\"X+Y\"] = combined[\"X\"] + combined[\"Y\"]\n",
    "combined[\"X-Y\"] = combined[\"X\"] - combined[\"Y\"]\n",
    "\n",
    "# combined[\"XY45_1\"] = combined[\"X\"] * np.cos(np.pi / 4) + combined[\"Y\"] * np.sin(np.pi / 4)\n",
    "combined[\"XY45_2\"] = combined[\"Y\"] * np.cos(np.pi / 4) - combined[\"X\"] * np.sin(np.pi / 4)\n",
    "\n",
    "combined[\"XY30_1\"] = combined[\"X\"] * np.cos(np.pi / 6) + combined[\"Y\"] * np.sin(np.pi / 6)\n",
    "combined[\"XY30_2\"] = combined[\"Y\"] * np.cos(np.pi / 6) - combined[\"X\"] * np.sin(np.pi / 6)\n",
    "\n",
    "combined[\"XY60_1\"] = combined[\"X\"] * np.cos(np.pi / 3) + combined[\"Y\"] * np.sin(np.pi / 3)\n",
    "combined[\"XY60_2\"] = combined[\"Y\"] * np.cos(np.pi / 3) - combined[\"X\"] * np.sin(np.pi / 3)\n",
    "\n",
    "\n",
    "combined[\"XY1\"] = (combined[\"X\"] - combined[\"X\"].min()) ** 2 + (combined[\"Y\"] - combined[\"Y\"].min()) ** 2\n",
    "combined[\"XY2\"] = (combined[\"X\"].max() - combined[\"X\"]) ** 2 + (combined[\"Y\"] - combined[\"Y\"].min()) ** 2\n",
    "combined[\"XY3\"] = (combined[\"X\"] - combined[\"X\"].min()) ** 2 + (combined[\"Y\"].max() - combined[\"Y\"]) ** 2\n",
    "combined[\"XY4\"] = (combined[\"X\"].max() - combined[\"X\"]) ** 2 + (combined[\"Y\"].max() - combined[\"Y\"]) ** 2\n",
    "combined[\"XY5\"] = (combined[\"X\"] - X_median) ** 2 + (combined[\"Y\"] - Y_median) ** 2\n",
    "\n",
    "combined[\"XY_rad\"] = np.sqrt(np.power(combined['Y'], 2) + np.power(combined['X'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2).fit(combined[[\"X\", \"Y\"]])\n",
    "XYt = pca.transform(combined[[\"X\", \"Y\"]])\n",
    "\n",
    "combined[\"XYpca1\"] = XYt[:, 0]\n",
    "combined[\"XYpca2\"] = XYt[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clf = GaussianMixture(n_components=150, covariance_type=\"diag\",\n",
    "                      random_state=0).fit(combined[[\"X\", \"Y\"]])\n",
    "combined[\"XYcluster\"] = clf.predict(combined[[\"X\", \"Y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Odds\n",
    "\n",
    "NOTE:  Log odds are also calculated for each individual crime category for each of the groupings below. The resulting values included a large amount of redundancy and hurt model performance, so they were omitted.\n",
    "\n",
    "### Log Odds - Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "\n",
    "addresses = sorted(train_df['Address'].unique())\n",
    "categories = sorted(train_df['Category'].unique())\n",
    "\n",
    "C_counts = train_df.groupby(['Category']).size()\n",
    "A_C_counts = train_df.groupby(['Address', 'Category']).size()\n",
    "A_counts = train_df.groupby(['Address']).size()\n",
    "\n",
    "logodds = {}\n",
    "logoddsPA = {}\n",
    "\n",
    "MIN_CAT_COUNTS = 2\n",
    "\n",
    "default_logodds = np.log(C_counts / len(train_df)) - np.log(1.0 - C_counts / float(len(train_df)))\n",
    "\n",
    "for addr in addresses:\n",
    "    \n",
    "    PA = A_counts[addr] / float(len(train_df))\n",
    "    logoddsPA[addr] = np.log(PA)- np.log(1.-PA)\n",
    "    logodds[addr] = deepcopy(default_logodds)\n",
    "    \n",
    "    for cat in A_C_counts[addr].keys():        \n",
    "        if (A_C_counts[addr][cat] > MIN_CAT_COUNTS) and A_C_counts[addr][cat] < A_counts[addr]:\n",
    "            PA = A_C_counts[addr][cat] / float(A_counts[addr])\n",
    "            logodds[addr][categories.index(cat)] = np.log(PA) - np.log(1.0-PA)\n",
    "\n",
    "    logodds[addr] = pd.Series(logodds[addr])\n",
    "    logodds[addr].index = range(len(categories))\n",
    "    \n",
    "# Test Data\n",
    "\n",
    "new_addresses = sorted(test_df[\"Address\"].unique())\n",
    "new_A_counts = test_df.groupby(\"Address\").size()\n",
    "\n",
    "only_new = set(new_addresses + addresses) - set(addresses)\n",
    "only_old = set(new_addresses + addresses) - set(new_addresses)\n",
    "in_both = set(new_addresses).intersection(addresses)\n",
    "\n",
    "for addr in only_new:\n",
    "    PA = new_A_counts[addr] / float(len(test_df) + len(train_df))\n",
    "    logoddsPA[addr] = np.log(PA) - np.log(1.0 - PA)\n",
    "    logodds[addr] = deepcopy(default_logodds)\n",
    "    logodds[addr].index = range(len(categories))\n",
    "for addr in in_both:\n",
    "    PA = (A_counts[addr] + new_A_counts[addr]) / float(len(test_df) + len(train_df))\n",
    "    logoddsPA[addr] = np.log(PA) - np.log(1.0 - PA)\n",
    "    \n",
    "address_features = combined['Address'].apply(lambda x: logodds[x])\n",
    "address_features.columns = ['Address Logodds ' + str(x) for x in range(len(address_features.columns))]\n",
    "combined[\"logoddsPA\"] = combined[\"Address\"].apply(lambda x: logoddsPA[x])\n",
    "\n",
    "# combined = pd.concat([combined, address_features], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Odds - Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_df = process_dates(train_df)\n",
    "\n",
    "hours = sorted(train_df['Hour'].unique())\n",
    "# categories = sorted(train_df['Category'].unique())\n",
    "\n",
    "# C_counts = train_df.groupby(['Category']).size()\n",
    "H_C_counts = train_df.groupby(['Hour', 'Category']).size()\n",
    "H_counts = train_df.groupby(['Hour']).size()\n",
    "\n",
    "hour_logodds = {}\n",
    "hour_logoddsPA = {}\n",
    "\n",
    "MIN_CAT_COUNTS = 2\n",
    "\n",
    "default_hour_logodds = np.log(C_counts / len(train_df)) - np.log(1.0 - C_counts / float(len(train_df)))\n",
    "\n",
    "for hr in hours:\n",
    "    \n",
    "    PH = H_counts[hr] / float(len(train_df))\n",
    "    hour_logoddsPA[hr] = np.log(PH)- np.log(1.-PH)\n",
    "    hour_logodds[hr] = deepcopy(default_hour_logodds)\n",
    "    \n",
    "    for cat in H_C_counts[hr].keys():        \n",
    "        if (H_C_counts[hr][cat] > MIN_CAT_COUNTS) and H_C_counts[hr][cat] < H_counts[hr]:\n",
    "            PH = H_C_counts[hr][cat] / float(H_counts[hr])\n",
    "            hour_logodds[hr][categories.index(cat)] = np.log(PH) - np.log(1.0-PH)\n",
    "\n",
    "    hour_logodds[hr] = pd.Series(hour_logodds[hr])\n",
    "    hour_logodds[hr].index = range(len(categories))\n",
    "    \n",
    "# Test Data\n",
    "test_df = process_dates(test_df)\n",
    "\n",
    "new_hours = sorted(test_df[\"Hour\"].unique())\n",
    "new_H_counts = test_df.groupby(\"Hour\").size()\n",
    "\n",
    "only_new = set(new_hours + hours) - set(hours)\n",
    "only_old = set(new_hours + hours) - set(new_hours)\n",
    "in_both = set(new_hours).intersection(hours)\n",
    "\n",
    "for hr in only_new:\n",
    "    PH = new_H_counts[hr] / float(len(test_df) + len(train_df))\n",
    "    hour_logoddsPA[hr] = np.log(PH) - np.log(1.0 - PH)\n",
    "    hour_logodds[hr] = deepcopy(default_hour_logodds)\n",
    "    hour_logodds[hr].index = range(len(categories))\n",
    "for hr in in_both:\n",
    "    PH = (H_counts[hr] + new_H_counts[hr]) / float(len(test_df) + len(train_df))\n",
    "    hour_logoddsPA[hr] = np.log(PH) - np.log(1.0 - PH)\n",
    "\n",
    "hour_features = combined['Hour'].apply(lambda x: hour_logodds[x])\n",
    "hour_features.columns = ['Hour Logodds ' + str(x) for x in range(len(hour_features.columns))]\n",
    "combined[\"hour logoddsPA\"] = combined[\"Hour\"].apply(lambda x: hour_logoddsPA[x])\n",
    "\n",
    "# combined = pd.concat([combined, hour_features], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Odds - DayOfWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "\n",
    "dows = sorted(train_df['DayOfWeek'].unique())\n",
    "# categories = sorted(train_df['Category'].unique())\n",
    "\n",
    "# C_counts = train_df.groupby(['Category']).size()\n",
    "D_C_counts = train_df.groupby(['DayOfWeek', 'Category']).size()\n",
    "D_counts = train_df.groupby(['DayOfWeek']).size()\n",
    "\n",
    "dow_logodds = {}\n",
    "dow_logoddsPA = {}\n",
    "\n",
    "MIN_CAT_COUNTS = 2\n",
    "\n",
    "default_dow_logodds = np.log(C_counts / len(train_df)) - np.log(1.0 - C_counts / float(len(train_df)))\n",
    "\n",
    "for dow in dows:\n",
    "    \n",
    "    PD = D_counts[dow] / float(len(train_df))\n",
    "    dow_logoddsPA[dow] = np.log(PD)- np.log(1.-PD)\n",
    "    dow_logodds[dow] = deepcopy(default_dow_logodds)\n",
    "    \n",
    "    for cat in D_C_counts[dow].keys():        \n",
    "        if (D_C_counts[dow][cat] > MIN_CAT_COUNTS) and D_C_counts[dow][cat] < D_counts[dow]:\n",
    "            PD = D_C_counts[dow][cat] / float(D_counts[dow])\n",
    "            dow_logodds[dow][categories.index(cat)] = np.log(PD) - np.log(1.0-PD)\n",
    "\n",
    "    dow_logodds[dow] = pd.Series(dow_logodds[dow])\n",
    "    dow_logodds[dow].index = range(len(categories))\n",
    "    \n",
    "new_dows = sorted(test_df[\"DayOfWeek\"].unique())\n",
    "new_D_counts = test_df.groupby(\"DayOfWeek\").size()\n",
    "\n",
    "only_new = set(new_dows + dows) - set(dows)\n",
    "only_old = set(new_dows + dows) - set(new_dows)\n",
    "in_both = set(new_dows).intersection(dows)\n",
    "\n",
    "for dow in only_new:\n",
    "    PD = new_D_counts[dow] / float(len(test_df) + len(train_df))\n",
    "    dow_logoddsPD[dow] = np.log(PD) - np.log(1.0 - PD)\n",
    "    dow_logodds[dow] = deepcopy(default_dow_logodds)\n",
    "    dow_logodds[dow].index = range(len(categories))\n",
    "    \n",
    "for dow in in_both:\n",
    "    PD = (D_counts[dow] + new_D_counts[dow]) / float(len(test_df) + len(train_df))\n",
    "    dow_logoddsPA[dow] = np.log(PD) - np.log(1.0 - PD)\n",
    "    \n",
    "dow_features = combined['DayOfWeek'].apply(lambda x: dow_logodds[x])\n",
    "dow_features.columns = ['DOW Logodds ' + str(x) for x in range(len(dow_features.columns))]\n",
    "combined[\"dow logoddsPA\"] = combined[\"DayOfWeek\"].apply(lambda x: dow_logoddsPA[x])\n",
    "\n",
    "# combined = pd.concat([combined, dow_features], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Address Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cols = []\n",
    "\n",
    "for i in range(encoded_address.shape[1]):\n",
    "    enc_cols.append(\"EncodedAddress{}\".format(i))\n",
    "    \n",
    "enc_add_df = pd.DataFrame(encoded_address, columns=enc_cols)\n",
    "\n",
    "combined = pd.concat([combined, enc_add_df], axis=1, sort=False)\n",
    "\n",
    "combined.drop('Address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"DayOfWeek\", \"PdDistrict\", \"Intersection\", \"Special Time\", \"XYcluster\"]\n",
    "\n",
    "for col in combined.columns:\n",
    "    if col in categorical_features:\n",
    "        oe = OrdinalEncoder()\n",
    "        combined[col] = oe.fit_transform(combined[col].values.reshape(-1,1))\n",
    "        combined[col] = combined[col].astype(int)\n",
    "    elif combined.dtypes[col] == 'object':\n",
    "        le = LabelEncoder()\n",
    "        combined[col] = le.fit_transform(combined[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined['HourCos'] = combined['Hour'].apply(lambda x: np.cos(x*2*np.pi)/24) \n",
    "# combined['DayOfWeekCos'] = combined['DayOfWeek'].apply(lambda x: np.cos(x*2*np.pi)/7) \n",
    "# combined['MonthCos'] = combined['Month'].apply(lambda x: np.cos(x*2*np.pi)/12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Back Into Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined[:train_length]\n",
    "X_test = combined[train_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization for Neural Network\n",
    "\n",
    "NOTE:  This step should be done within each cross validation fold in the stacking script instead of on the whole dataset. We will update it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "X_std = pd.DataFrame(X_std, columns=combined.columns)\n",
    "X_test_std = pd.DataFrame(X_test_std, columns=combined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Preprocessed Training / Test Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7b7d834edb08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         )\n\u001b[0;32m-> 3228\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    354\u001b[0m         )\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_label = pd.DataFrame(y_label, columns=['Category'])\n",
    "\n",
    "X.to_csv('p_train.csv', index=False)\n",
    "X_test.to_csv('p_test.csv', index=False)\n",
    "\n",
    "# y_nn.to_csv('p_train_y_nn.csv', index=False)\n",
    "# y_label.to_csv('p_train_y_label.csv', index=False)\n",
    "\n",
    "X_std.to_csv('p_train_std.csv', index=False)\n",
    "X_test_std.to_csv('p_test_std.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
