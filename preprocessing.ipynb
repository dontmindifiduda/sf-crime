{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Script\n",
    "\n",
    "This script takes train.csv and test.csv as input and generates the following preprocessed output files:\n",
    "\n",
    "- p_train.csv - training data\n",
    "- p_test.csv - test data\n",
    "- p_train_y_nn.csv - training data labels processed for neural network\n",
    "- p_train_y_label.csv - training data labels processed for tree-based models\n",
    "- p_train_std.csv - standardized training data for neural network\n",
    "- p_test_std.csv - standardized test data for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "\n",
    "import gensim\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', low_memory=False)  # update path as needed\n",
    "test_df = pd.read_csv('test.csv', low_memory=False) # update path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Features from 'Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dates(df):\n",
    "    \n",
    "    df['Dates'] = df['Dates'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "    df['Year'] = df['Dates'].apply(lambda x: x.year)\n",
    "    df['Month'] = df['Dates'].apply(lambda x: x.month)\n",
    "    df['Day'] = df['Dates'].apply(lambda x: x.day)\n",
    "    df['Hour'] = df['Dates'].apply(lambda x: x.hour)\n",
    "    df['Minute'] = df['Dates'].apply(lambda x: x.minute)\n",
    "    df['Special Time'] = df['Minute'].isin([0,30]).astype(int)\n",
    "    df.drop('Dates', axis=1, inplace=True)\n",
    "    \n",
    "    df['Weekend'] = df['DayOfWeek'].apply(lambda x: 1 if x == 'Saturday' or x == 'Sunday' else 0)\n",
    "    df['Night'] = df['Hour'].apply(lambda x: 1 if x > 6 and x < 18 else 0)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicate Training Entries and Unuseful Columns, Manage Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "train_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage outliers\n",
    "train_df.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "test_df.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "\n",
    "for district in train_df['PdDistrict'].unique():\n",
    "    train_df.loc[train_df['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n",
    "        train_df.loc[train_df['PdDistrict'] == district, ['X', 'Y']])\n",
    "    test_df.loc[test_df['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n",
    "        test_df.loc[test_df['PdDistrict'] == district, ['X', 'Y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unuseful columns \n",
    "\n",
    "drop_cols = ['Descript', 'Resolution', 'Id']\n",
    "\n",
    "for col in drop_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df.drop(col, axis=1, inplace=True)\n",
    "    if col in test_df.columns:\n",
    "        test_df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "X = train_df.drop('Category', axis=1)\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cats = train_df['Category']\n",
    "unique_cats = np.sort(y_cats.unique())\n",
    "\n",
    "# neural network\n",
    "y = np.zeros((y_cats.shape[0], 39))\n",
    "for idx, target in enumerate(list(y_cats)):\n",
    "    y[idx, np.where(unique_cats == target)] = 1\n",
    "\n",
    "y_nn = pd.DataFrame(y, columns = unique_cats)\n",
    "\n",
    "\n",
    "# tree-based models\n",
    "y_label = train_df['Category']\n",
    "le = LabelEncoder()\n",
    "y_label = le.fit_transform(y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = X.shape[0]\n",
    "\n",
    "combined = pd.concat([X, X_test], ignore_index=True)\n",
    "combined = process_dates(combined)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Address Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_list = [address.split(' ') for address in combined['Address']]\n",
    "address_model = gensim.models.Word2Vec(address_list, min_count=1)\n",
    "encoded_address = np.zeros((combined.shape[0], 100))\n",
    "for i in range(len(address_list)):\n",
    "    for j in range(len(address_list[i])):\n",
    "        encoded_address[i] += address_model.wv[address_list[i][j]]\n",
    "    encoded_address[i] /= len(address_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Intersection'] = combined['Address'].apply(lambda x: 1 if '/' in x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations & Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xy_scaler = StandardScaler()\n",
    "# xy_scaler.fit(combined[['X', 'Y']])\n",
    "# combined[['X', 'Y']] = xy_scaler.transform(combined[['X', 'Y']])\n",
    "\n",
    "X_median = combined[\"X\"].median()\n",
    "Y_median = combined[\"Y\"].median()\n",
    "\n",
    "combined[\"X+Y\"] = combined[\"X\"] + combined[\"Y\"]\n",
    "combined[\"X-Y\"] = combined[\"X\"] - combined[\"Y\"]\n",
    "\n",
    "# combined[\"XY45_1\"] = combined[\"X\"] * np.cos(np.pi / 4) + combined[\"Y\"] * np.sin(np.pi / 4)\n",
    "combined[\"XY45_2\"] = combined[\"Y\"] * np.cos(np.pi / 4) - combined[\"X\"] * np.sin(np.pi / 4)\n",
    "\n",
    "combined[\"XY30_1\"] = combined[\"X\"] * np.cos(np.pi / 6) + combined[\"Y\"] * np.sin(np.pi / 6)\n",
    "combined[\"XY30_2\"] = combined[\"Y\"] * np.cos(np.pi / 6) - combined[\"X\"] * np.sin(np.pi / 6)\n",
    "\n",
    "combined[\"XY60_1\"] = combined[\"X\"] * np.cos(np.pi / 3) + combined[\"Y\"] * np.sin(np.pi / 3)\n",
    "combined[\"XY60_2\"] = combined[\"Y\"] * np.cos(np.pi / 3) - combined[\"X\"] * np.sin(np.pi / 3)\n",
    "\n",
    "\n",
    "combined[\"XY1\"] = (combined[\"X\"] - combined[\"X\"].min()) ** 2 + (combined[\"Y\"] - combined[\"Y\"].min()) ** 2\n",
    "combined[\"XY2\"] = (combined[\"X\"].max() - combined[\"X\"]) ** 2 + (combined[\"Y\"] - combined[\"Y\"].min()) ** 2\n",
    "combined[\"XY3\"] = (combined[\"X\"] - combined[\"X\"].min()) ** 2 + (combined[\"Y\"].max() - combined[\"Y\"]) ** 2\n",
    "combined[\"XY4\"] = (combined[\"X\"].max() - combined[\"X\"]) ** 2 + (combined[\"Y\"].max() - combined[\"Y\"]) ** 2\n",
    "combined[\"XY5\"] = (combined[\"X\"] - X_median) ** 2 + (combined[\"Y\"] - Y_median) ** 2\n",
    "\n",
    "combined[\"XY_rad\"] = np.sqrt(np.power(combined['Y'], 2) + np.power(combined['X'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2).fit(combined[[\"X\", \"Y\"]])\n",
    "XYt = pca.transform(combined[[\"X\", \"Y\"]])\n",
    "\n",
    "combined[\"XYpca1\"] = XYt[:, 0]\n",
    "combined[\"XYpca2\"] = XYt[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-66726c3d6e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m clf = GaussianMixture(n_components=150, covariance_type=\"diag\",\n\u001b[0;32m----> 4\u001b[0;31m                       random_state=0).fit(combined[[\"X\", \"Y\"]])\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"XYcluster\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfty\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bound_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36m_initialize_parameters\u001b[0;34m(self, X, random_state)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n\u001b[0;32m--> 147\u001b[0;31m                                    random_state=random_state).fit(X).labels_\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_params\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                     \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                     x_squared_norms=x_squared_norms, random_state=seed)\n\u001b[0m\u001b[1;32m    938\u001b[0m                 \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    320\u001b[0m     centers, labels, n_iter = k_means_elkan(X, checked_sample_weight,\n\u001b[1;32m    321\u001b[0m                                             \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/cluster/_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;31m# Pairwise distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,\n\u001b[0m\u001b[1;32m    196\u001b[0m                         X_norm_squared=None):\n\u001b[1;32m    197\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clf = GaussianMixture(n_components=150, covariance_type=\"diag\",\n",
    "                      random_state=0).fit(combined[[\"X\", \"Y\"]])\n",
    "combined[\"XYcluster\"] = clf.predict(combined[[\"X\", \"Y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Odds\n",
    "\n",
    "NOTE:  Log odds are also calculated for each individual crime category for each of the groupings below. The resulting values included a large amount of redundancy and hurt model performance, so they were omitted.\n",
    "\n",
    "### Log Odds - Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "\n",
    "addresses = sorted(train_df['Address'].unique())\n",
    "categories = sorted(train_df['Category'].unique())\n",
    "\n",
    "C_counts = train_df.groupby(['Category']).size()\n",
    "A_C_counts = train_df.groupby(['Address', 'Category']).size()\n",
    "A_counts = train_df.groupby(['Address']).size()\n",
    "\n",
    "logodds = {}\n",
    "logoddsPA = {}\n",
    "\n",
    "MIN_CAT_COUNTS = 2\n",
    "\n",
    "default_logodds = np.log(C_counts / len(train_df)) - np.log(1.0 - C_counts / float(len(train_df)))\n",
    "\n",
    "for addr in addresses:\n",
    "    \n",
    "    PA = A_counts[addr] / float(len(train_df))\n",
    "    logoddsPA[addr] = np.log(PA)- np.log(1.-PA)\n",
    "    logodds[addr] = deepcopy(default_logodds)\n",
    "    \n",
    "    for cat in A_C_counts[addr].keys():        \n",
    "        if (A_C_counts[addr][cat] > MIN_CAT_COUNTS) and A_C_counts[addr][cat] < A_counts[addr]:\n",
    "            PA = A_C_counts[addr][cat] / float(A_counts[addr])\n",
    "            logodds[addr][categories.index(cat)] = np.log(PA) - np.log(1.0-PA)\n",
    "\n",
    "    logodds[addr] = pd.Series(logodds[addr])\n",
    "    logodds[addr].index = range(len(categories))\n",
    "    \n",
    "# Test Data\n",
    "\n",
    "new_addresses = sorted(test_df[\"Address\"].unique())\n",
    "new_A_counts = test_df.groupby(\"Address\").size()\n",
    "\n",
    "only_new = set(new_addresses + addresses) - set(addresses)\n",
    "only_old = set(new_addresses + addresses) - set(new_addresses)\n",
    "in_both = set(new_addresses).intersection(addresses)\n",
    "\n",
    "for addr in only_new:\n",
    "    PA = new_A_counts[addr] / float(len(test_df) + len(train_df))\n",
    "    logoddsPA[addr] = np.log(PA) - np.log(1.0 - PA)\n",
    "    logodds[addr] = deepcopy(default_logodds)\n",
    "    logodds[addr].index = range(len(categories))\n",
    "for addr in in_both:\n",
    "    PA = (A_counts[addr] + new_A_counts[addr]) / float(len(test_df) + len(train_df))\n",
    "    logoddsPA[addr] = np.log(PA) - np.log(1.0 - PA)\n",
    "    \n",
    "address_features = combined['Address'].apply(lambda x: logodds[x])\n",
    "address_features.columns = ['Address Logodds ' + str(x) for x in range(len(address_features.columns))]\n",
    "combined[\"logoddsPA\"] = combined[\"Address\"].apply(lambda x: logoddsPA[x])\n",
    "\n",
    "# combined = pd.concat([combined, address_features], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Odds - Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_df = process_dates(train_df)\n",
    "\n",
    "hours = sorted(train_df['Hour'].unique())\n",
    "# categories = sorted(train_df['Category'].unique())\n",
    "\n",
    "# C_counts = train_df.groupby(['Category']).size()\n",
    "H_C_counts = train_df.groupby(['Hour', 'Category']).size()\n",
    "H_counts = train_df.groupby(['Hour']).size()\n",
    "\n",
    "hour_logodds = {}\n",
    "hour_logoddsPA = {}\n",
    "\n",
    "MIN_CAT_COUNTS = 2\n",
    "\n",
    "default_hour_logodds = np.log(C_counts / len(train_df)) - np.log(1.0 - C_counts / float(len(train_df)))\n",
    "\n",
    "for hr in hours:\n",
    "    \n",
    "    PH = H_counts[hr] / float(len(train_df))\n",
    "    hour_logoddsPA[hr] = np.log(PH)- np.log(1.-PH)\n",
    "    hour_logodds[hr] = deepcopy(default_hour_logodds)\n",
    "    \n",
    "    for cat in H_C_counts[hr].keys():        \n",
    "        if (H_C_counts[hr][cat] > MIN_CAT_COUNTS) and H_C_counts[hr][cat] < H_counts[hr]:\n",
    "            PH = H_C_counts[hr][cat] / float(H_counts[hr])\n",
    "            hour_logodds[hr][categories.index(cat)] = np.log(PH) - np.log(1.0-PH)\n",
    "\n",
    "    hour_logodds[hr] = pd.Series(hour_logodds[hr])\n",
    "    hour_logodds[hr].index = range(len(categories))\n",
    "    \n",
    "# Test Data\n",
    "test_df = process_dates(test_df)\n",
    "\n",
    "new_hours = sorted(test_df[\"Hour\"].unique())\n",
    "new_H_counts = test_df.groupby(\"Hour\").size()\n",
    "\n",
    "only_new = set(new_hours + hours) - set(hours)\n",
    "only_old = set(new_hours + hours) - set(new_hours)\n",
    "in_both = set(new_hours).intersection(hours)\n",
    "\n",
    "for hr in only_new:\n",
    "    PH = new_H_counts[hr] / float(len(test_df) + len(train_df))\n",
    "    hour_logoddsPA[hr] = np.log(PH) - np.log(1.0 - PH)\n",
    "    hour_logodds[hr] = deepcopy(default_hour_logodds)\n",
    "    hour_logodds[hr].index = range(len(categories))\n",
    "for hr in in_both:\n",
    "    PH = (H_counts[hr] + new_H_counts[hr]) / float(len(test_df) + len(train_df))\n",
    "    hour_logoddsPA[hr] = np.log(PH) - np.log(1.0 - PH)\n",
    "\n",
    "hour_features = combined['Hour'].apply(lambda x: hour_logodds[x])\n",
    "hour_features.columns = ['Hour Logodds ' + str(x) for x in range(len(hour_features.columns))]\n",
    "combined[\"hour logoddsPA\"] = combined[\"Hour\"].apply(lambda x: hour_logoddsPA[x])\n",
    "\n",
    "# combined = pd.concat([combined, hour_features], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Odds - DayOfWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "\n",
    "dows = sorted(train_df['DayOfWeek'].unique())\n",
    "# categories = sorted(train_df['Category'].unique())\n",
    "\n",
    "# C_counts = train_df.groupby(['Category']).size()\n",
    "D_C_counts = train_df.groupby(['DayOfWeek', 'Category']).size()\n",
    "D_counts = train_df.groupby(['DayOfWeek']).size()\n",
    "\n",
    "dow_logodds = {}\n",
    "dow_logoddsPA = {}\n",
    "\n",
    "MIN_CAT_COUNTS = 2\n",
    "\n",
    "default_dow_logodds = np.log(C_counts / len(train_df)) - np.log(1.0 - C_counts / float(len(train_df)))\n",
    "\n",
    "for dow in dows:\n",
    "    \n",
    "    PD = D_counts[dow] / float(len(train_df))\n",
    "    dow_logoddsPA[dow] = np.log(PD)- np.log(1.-PD)\n",
    "    dow_logodds[dow] = deepcopy(default_dow_logodds)\n",
    "    \n",
    "    for cat in D_C_counts[dow].keys():        \n",
    "        if (D_C_counts[dow][cat] > MIN_CAT_COUNTS) and D_C_counts[dow][cat] < D_counts[dow]:\n",
    "            PD = D_C_counts[dow][cat] / float(D_counts[dow])\n",
    "            dow_logodds[dow][categories.index(cat)] = np.log(PD) - np.log(1.0-PD)\n",
    "\n",
    "    dow_logodds[dow] = pd.Series(dow_logodds[dow])\n",
    "    dow_logodds[dow].index = range(len(categories))\n",
    "    \n",
    "new_dows = sorted(test_df[\"DayOfWeek\"].unique())\n",
    "new_D_counts = test_df.groupby(\"DayOfWeek\").size()\n",
    "\n",
    "only_new = set(new_dows + dows) - set(dows)\n",
    "only_old = set(new_dows + dows) - set(new_dows)\n",
    "in_both = set(new_dows).intersection(dows)\n",
    "\n",
    "for dow in only_new:\n",
    "    PD = new_D_counts[dow] / float(len(test_df) + len(train_df))\n",
    "    dow_logoddsPD[dow] = np.log(PD) - np.log(1.0 - PD)\n",
    "    dow_logodds[dow] = deepcopy(default_dow_logodds)\n",
    "    dow_logodds[dow].index = range(len(categories))\n",
    "    \n",
    "for dow in in_both:\n",
    "    PD = (D_counts[dow] + new_D_counts[dow]) / float(len(test_df) + len(train_df))\n",
    "    dow_logoddsPA[dow] = np.log(PD) - np.log(1.0 - PD)\n",
    "    \n",
    "dow_features = combined['DayOfWeek'].apply(lambda x: dow_logodds[x])\n",
    "dow_features.columns = ['DOW Logodds ' + str(x) for x in range(len(dow_features.columns))]\n",
    "combined[\"dow logoddsPA\"] = combined[\"DayOfWeek\"].apply(lambda x: dow_logoddsPA[x])\n",
    "\n",
    "# combined = pd.concat([combined, dow_features], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Address Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cols = []\n",
    "\n",
    "for i in range(encoded_address.shape[1]):\n",
    "    enc_cols.append(\"EncodedAddress{}\".format(i))\n",
    "    \n",
    "enc_add_df = pd.DataFrame(encoded_address, columns=enc_cols)\n",
    "\n",
    "combined = pd.concat([combined, enc_add_df], axis=1, sort=False)\n",
    "\n",
    "combined.drop('Address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"DayOfWeek\", \"PdDistrict\", \"Intersection\", \"Special Time\", \"XYcluster\"]\n",
    "\n",
    "for col in combined.columns:\n",
    "    if col in categorical_features:\n",
    "        oe = OrdinalEncoder()\n",
    "        combined[col] = oe.fit_transform(combined[col].values.reshape(-1,1))\n",
    "        combined[col] = combined[col].astype(int)\n",
    "    elif combined.dtypes[col] == 'object':\n",
    "        le = LabelEncoder()\n",
    "        combined[col] = le.fit_transform(combined[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined['HourCos'] = combined['Hour'].apply(lambda x: np.cos(x*2*np.pi)/24) \n",
    "# combined['DayOfWeekCos'] = combined['DayOfWeek'].apply(lambda x: np.cos(x*2*np.pi)/7) \n",
    "# combined['MonthCos'] = combined['Month'].apply(lambda x: np.cos(x*2*np.pi)/12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Back Into Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined[:train_length]\n",
    "X_test = combined[train_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization for Neural Network\n",
    "\n",
    "NOTE:  This step should be done within each cross validation fold in the stacking script instead of on the whole dataset. We will update it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "X_std = pd.DataFrame(X_std, columns=combined.columns)\n",
    "X_test_std = pd.DataFrame(X_test_std, columns=combined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Preprocessed Training / Test Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = pd.DataFrame(y_label, columns=['Category'])\n",
    "\n",
    "X.to_csv('p_train.csv', index=False)\n",
    "X_test.to_csv('p_test.csv', index=False)\n",
    "\n",
    "X_std.to_csv('p_train_std.csv', index=False)\n",
    "X_test_std.to_csv('p_test_std.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
